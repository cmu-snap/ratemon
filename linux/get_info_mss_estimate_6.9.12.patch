commit 399dacadc7027ee26b71ec072a54a3e61746709a
Author: Christopher Canel <c.canel@icloud.com>
Date:   Tue Oct 14 00:14:59 2025 -0700

    Enable get_info; cap TCP MSS estimate

diff --git a/include/net/netns/ipv4.h b/include/net/netns/ipv4.h
index c356c458b..425c9b36f 100644
--- a/include/net/netns/ipv4.h
+++ b/include/net/netns/ipv4.h
@@ -150,6 +150,7 @@ struct netns_ipv4 {
 	int sysctl_tcp_probe_threshold;
 	u32 sysctl_tcp_probe_interval;
 
+	int sysctl_tcp_mss_estimate;
 	int sysctl_tcp_keepalive_time;
 	int sysctl_tcp_keepalive_intvl;
 	u8 sysctl_tcp_keepalive_probes;
diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2bcf30381..b0996ddb7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -158,6 +158,7 @@ static_assert((1 << ATO_BITS) > TCP_DELACK_MAX);
 #define TCP_RESOURCE_PROBE_INTERVAL ((unsigned)(HZ/2U)) /* Maximal interval between probes
 					                 * for local resources.
 					                 */
+#define TCP_MSS_ESTIMATE	65536		/* bytes */
 #define TCP_KEEPALIVE_TIME	(120*60*HZ)	/* two hours */
 #define TCP_KEEPALIVE_PROBES	9		/* Max of 9 keepalive probes	*/
 #define TCP_KEEPALIVE_INTVL	(75*HZ)
diff --git a/include/trace/events/tcp.h b/include/trace/events/tcp.h
index 699dafd20..09143358d 100644
--- a/include/trace/events/tcp.h
+++ b/include/trace/events/tcp.h
@@ -427,6 +427,58 @@ TRACE_EVENT(tcp_cong_state_set,
 		  __entry->cong_state)
 );
 
+TRACE_EVENT(tcp_mss_estimate_capped,
+
+	TP_PROTO(struct sock *sk, unsigned int orig_mss, unsigned int capped_mss, unsigned int cap_value),
+
+	TP_ARGS(sk, orig_mss, capped_mss, cap_value),
+
+	TP_STRUCT__entry(
+		__field(const void *, skaddr)
+		__field(__u16, sport)
+		__field(__u16, dport)
+		__field(__u16, family)
+		__array(__u8, saddr, 4)
+		__array(__u8, daddr, 4)
+		__array(__u8, saddr_v6, 16)
+		__array(__u8, daddr_v6, 16)
+		__field(__u32, orig_mss)
+		__field(__u32, capped_mss)
+		__field(__u32, cap_value)
+	),
+
+	TP_fast_assign(
+		struct inet_sock *inet = inet_sk(sk);
+		__be32 *p32;
+
+		__entry->skaddr = sk;
+		__entry->sport = ntohs(inet->inet_sport);
+		__entry->dport = ntohs(inet->inet_dport);
+		__entry->family = sk->sk_family;
+
+		p32 = (__be32 *) __entry->saddr;
+		*p32 = inet->inet_saddr;
+
+		p32 = (__be32 *) __entry->daddr;
+		*p32 = inet->inet_daddr;
+
+		TP_STORE_ADDRS(__entry, inet->inet_saddr, inet->inet_daddr,
+			      sk->sk_v6_rcv_saddr, sk->sk_v6_daddr);
+
+		__entry->orig_mss = orig_mss;
+		__entry->capped_mss = capped_mss;
+		__entry->cap_value = cap_value;
+	),
+
+	TP_printk("skaddr=%p family=%s sport=%hu dport=%hu saddr=%pI4 daddr=%pI4 saddrv6=%pI6c daddrv6=%pI6c orig_mss=%u capped_mss=%u cap_value=%u",
+		  __entry->skaddr,
+		  show_family_name(__entry->family),
+		  __entry->sport, __entry->dport,
+		  __entry->saddr, __entry->daddr,
+		  __entry->saddr_v6, __entry->daddr_v6,
+		  __entry->orig_mss, __entry->capped_mss, __entry->cap_value)
+);
+
 #endif /* _TRACE_TCP_H */
 
 /* This part must be outside protection */
diff --git a/net/ipv4/bpf_tcp_ca.c b/net/ipv4/bpf_tcp_ca.c
index 7f518ea5f..edccafb1a 100644
--- a/net/ipv4/bpf_tcp_ca.c
+++ b/net/ipv4/bpf_tcp_ca.c
@@ -15,7 +15,6 @@
 static struct bpf_struct_ops bpf_tcp_congestion_ops;
 
 static u32 unsupported_ops[] = {
-	offsetof(struct tcp_congestion_ops, get_info),
 };
 
 static const struct btf_type *tcp_sock_type;
@@ -321,6 +320,11 @@ static u32 bpf_tcp_ca_sndbuf_expand(struct sock *sk)
 	return 0;
 }
 
+static size_t bpf_tcp_ca_get_info(struct sock *sk, __u32 ext, int *attr, union tcp_cc_info *info)
+{
+	return 0;
+}
+
 static void __bpf_tcp_ca_init(struct sock *sk)
 {
 }
@@ -340,6 +344,7 @@ static struct tcp_congestion_ops __bpf_ops_tcp_congestion_ops = {
 	.cong_control = bpf_tcp_ca_cong_control,
 	.undo_cwnd = bpf_tcp_ca_undo_cwnd,
 	.sndbuf_expand = bpf_tcp_ca_sndbuf_expand,
+	.get_info = bpf_tcp_ca_get_info,
 
 	.init = __bpf_tcp_ca_init,
 	.release = __bpf_tcp_ca_release,
diff --git a/net/ipv4/sysctl_net_ipv4.c b/net/ipv4/sysctl_net_ipv4.c
index 7e4f16a7d..4b82465ca 100644
--- a/net/ipv4/sysctl_net_ipv4.c
+++ b/net/ipv4/sysctl_net_ipv4.c
@@ -902,6 +902,13 @@ static struct ctl_table ipv4_net_table[] = {
 		.mode		= 0644,
 		.proc_handler   = proc_allowed_congestion_control,
 	},
+	{
+		.procname	= "tcp_mss_estimate",
+		.data		= &init_net.ipv4.sysctl_tcp_mss_estimate,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+	},
 	{
 		.procname	= "tcp_keepalive_time",
 		.data		= &init_net.ipv4.sysctl_tcp_keepalive_time,
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index c765d4798..dfb593771 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -215,6 +215,24 @@ static __cold void tcp_gro_dev_warn(const struct sock *sk, const struct sk_buff
 	rcu_read_unlock();
 }
 
+/* If configured, cap the MSS estimate (rcv_mss) to a sysctl value. */
+static void maybe_cap_tcp_mss_estimate(struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	int mss_estimate_conf = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_mss_estimate);
+	if (mss_estimate_conf > 0) {
+		unsigned int orig_rcv_mss = icsk->icsk_ack.rcv_mss;
+		icsk->icsk_ack.rcv_mss = min_t(
+			unsigned int, icsk->icsk_ack.rcv_mss, mss_estimate_conf);
+
+		/* Trace when the cap is actually applied */
+		if (orig_rcv_mss != icsk->icsk_ack.rcv_mss) {
+			trace_tcp_mss_estimate_capped(sk, orig_rcv_mss, icsk->icsk_ack.rcv_mss,
+					      mss_estimate_conf);
+		}
+	}
+}
+
 /* Adapt the MSS value used to make delayed ack decision to the
  * real world.
  */
@@ -282,6 +300,7 @@ static void tcp_measure_rcv_mss(struct sock *sk, const struct sk_buff *skb)
 			icsk->icsk_ack.last_seg_size = len;
 			if (len == lss) {
 				icsk->icsk_ack.rcv_mss = len;
+				maybe_cap_tcp_mss_estimate(sk);
 				return;
 			}
 		}
@@ -289,6 +308,7 @@ static void tcp_measure_rcv_mss(struct sock *sk, const struct sk_buff *skb)
 			icsk->icsk_ack.pending |= ICSK_ACK_PUSHED2;
 		icsk->icsk_ack.pending |= ICSK_ACK_PUSHED;
 	}
+	maybe_cap_tcp_mss_estimate(sk);
 }
 
 static void tcp_incr_quickack(struct sock *sk, unsigned int max_quickacks)
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index 92511b7fd..1ff33e414 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -3422,6 +3422,8 @@ static int __net_init tcp_sk_init(struct net *net)
 	net->ipv4.sysctl_tcp_probe_interval = TCP_PROBE_INTERVAL;
 	net->ipv4.sysctl_tcp_mtu_probe_floor = TCP_MIN_SND_MSS;
 
+	net->ipv4.sysctl_tcp_mss_estimate = TCP_MSS_ESTIMATE;
+
 	net->ipv4.sysctl_tcp_keepalive_time = TCP_KEEPALIVE_TIME;
 	net->ipv4.sysctl_tcp_keepalive_probes = TCP_KEEPALIVE_PROBES;
 	net->ipv4.sysctl_tcp_keepalive_intvl = TCP_KEEPALIVE_INTVL;
diff --git a/tools/testing/selftests/bpf/bpf_tcp_helpers.h b/tools/testing/selftests/bpf/bpf_tcp_helpers.h
index 82a7c9de9..bad7ff36c 100644
--- a/tools/testing/selftests/bpf/bpf_tcp_helpers.h
+++ b/tools/testing/selftests/bpf/bpf_tcp_helpers.h
@@ -183,6 +183,9 @@ struct tcp_congestion_ops {
 	__u32 (*min_tso_segs)(struct sock *sk);
 	/* returns the multiplier used in tcp_sndbuf_expand (optional) */
 	__u32 (*sndbuf_expand)(struct sock *sk);
+	/* get info for inet_diag (optional) */
+	size_t (*get_info)(struct sock *sk, __u32 ext, int *attr,
+			   union tcp_cc_info *info);
 	/* call when packets are delivered to update cwnd and pacing rate,
 	 * after all the ca_state processing. (optional)
 	 */
diff --git a/tools/testing/selftests/bpf/progs/bpf_dctcp.c b/tools/testing/selftests/bpf/progs/bpf_dctcp.c
index 460682759..8adec6f39 100644
--- a/tools/testing/selftests/bpf/progs/bpf_dctcp.c
+++ b/tools/testing/selftests/bpf/progs/bpf_dctcp.c
@@ -11,6 +11,7 @@
 #include <linux/types.h>
 #include <linux/stddef.h>
 #include <linux/tcp.h>
+#include <linux/inet_diag.h>
 #include <errno.h>
 #include <bpf/bpf_helpers.h>
 #include <bpf/bpf_tracing.h>
@@ -228,6 +229,31 @@ __u32 BPF_PROG(dctcp_cwnd_undo, struct sock *sk)
 	return max(tcp_sk(sk)->snd_cwnd, ca->loss_cwnd);
 }
 
+SEC("struct_ops/dctcp_get_info")
+size_t BPF_PROG(dctcp_get_info, struct sock *sk, __u32 ext, int *attr, union tcp_cc_info *info)
+{
+	const struct dctcp *ca = inet_csk_ca(sk);
+	const struct tcp_sock *tp = tcp_sk(sk);
+
+	/* Fill it also in case of VEGASINFO due to req struct limits.
+	 * We can still correctly retrieve it later.
+	 */
+	if (ext & (1 << (INET_DIAG_DCTCPINFO - 1)) ||
+	    ext & (1 << (INET_DIAG_VEGASINFO - 1))) {
+		memset(&info->dctcp, 0, sizeof(info->dctcp));
+		info->dctcp.dctcp_enabled = 1;
+		info->dctcp.dctcp_ce_state = (__u16) ca->ce_state;
+		info->dctcp.dctcp_alpha = ca->dctcp_alpha;
+		info->dctcp.dctcp_ab_ecn = tp->mss_cache *
+						(tp->delivered_ce - ca->old_delivered_ce);
+		info->dctcp.dctcp_ab_tot = tp->mss_cache *
+						(tp->delivered - ca->old_delivered);
+		*attr = INET_DIAG_DCTCPINFO;
+		return sizeof(info->dctcp);
+	}
+	return 0;
+}
+
 extern void tcp_reno_cong_avoid(struct sock *sk, __u32 ack, __u32 acked) __ksym;
 
 SEC("struct_ops/dctcp_reno_cong_avoid")
@@ -253,6 +279,7 @@ struct tcp_congestion_ops dctcp = {
 	.cong_avoid	= (void *)dctcp_cong_avoid,
 	.undo_cwnd	= (void *)dctcp_cwnd_undo,
 	.set_state	= (void *)dctcp_state,
+	.set_state	= (void *)dctcp_get_info,
 	.flags		= TCP_CONG_NEEDS_ECN,
 	.name		= "bpf_dctcp",
 };
